The tall stack of software and hardware required for modern deep learning exposes many opportunites for optimization.
At the same time, its depth relative to its width tends to encourage research that focuses on narrow aspects of optimization.
In this thesis, we have attempted to expose a wide vertical section of the stack, though much more remains to be done both vertically and horizontally.

For example, it is not an accident that the dominant deep learning operators behind the most sophisticated and accurate models (across all domains) map exceptionally well to current hardware architectures and memory hierachies.
While fast hardware enabled many of the breakthroughs that have fueld the current popularity of and interest in deep learning, the current trend of increasingly specialized accelerators~\cite{chen2014diannao, chen2016diannao, jouppi2017datacenter, abts2020think} for a narrow set of workloads becomes worrying.


Others have noticed~\cite{barham2019machine} this trend, both across the hardware and software stacks.



