\paragraph{Improving Image Storage}
\input{storage/future.tex}

\paragraph{Data Augmentations for Pretraining}
Data augmentations are an interesting topic of study in unsupervised or semi-supervised learning settings.
From one perspective, a reasonable objective is to use augmentations to enforce consistency between perturbed input examples originating from the same source.
From another, augmentations that distort images can be potentially useful in creating pre-training tasks (e.g., correctly ordering shuffled image patches, reorienting a rotated image).
However, not all augmentations appear to be useful as components of pre-training tasks, as it may be desirable for the downstream fine-tuned model to normalize away such augmentations.
In these situations, it may be useful to enforce that neural networks remain equivariant to augmentation attributes, just as convolution has been classically motivated as a translation equivariant operator.


\paragraph{Solving the Scale Equivariance Problem}
Another approach to solving the scale invariance problem is to change the region of interest that is used as input to a computer vision model dynamically, depending on the positioning and scale of an object in a frame.
This approach would effectively be an extension of neural network models that use a version of the attention mechanism popular in natural language processing models.
While convolution-based architectures are currently more common in computer vision, recent work has shown that with sufficient computation, attention-based architectures can also be competitive with state-of-the-art convolution approaches.


\paragraph{Tuning Deep Learning Kernels}
While tremendous progress has been made in reducing the amount of human engineering effort needed to produce fast kernels for deep learning, much work remains to be done.
Current approaches have largely focused on a narrow range of dense linear-algebra inspired operators, although it is unclear whether these operators have been chosen simply because of the ease of mapping them efficiently to hardware or because of their suitability to deep learning architectures.
Optimizing arbitrary computation, especially on an end-to-end computation graph remains challenging, as even the subproblem of choosing how to slice the graph is nontrivial.

Perhaps the clearest example of "slicing" the graph is the current dichotomy between "graph optimization" and "kernel optimization."
Current approaches have demonstrated clear benefits by partioning different kernels in deep learning computation graphs.
However, these approaches still consider deep learning kernels as indivisible black boxes to tame the search spaceof possible and valid rewrites.
The development of future primitives is likely hamstrung by these limitations, as even arbitrary ``numpy'' style scripting remains difficult for optimization.

Finally, the need for any human insight into the structure of kernel implementations appears unsatisfying in the presence of ``tabula rasa''~\cite{silver2017mastering} approaches for reinforcement learning.
Even in the absence of template driven approaches, current state-of-the-art tuning methods require a considerable amount of human insight to design the search space~\cite{zheng2020ansor} or promising transformations available to the optimizer.
Ideally, to minimize programmer effort and maximize generalization, a ``tabula rasa'' approach would allow for competitive optimizations without specifying a constrained search space or narrow set of availble transformations tailored to specific architectures ahead of time.
