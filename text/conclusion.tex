\paragraph{CNNs Encode Data Augmentations}
Typical convolutional neural networks encode attributes corresponding to data augmentations such as object scale, aspect ratio, and various color transformations.
However, the signal augmentation is most prevalent in the early layers of models, with the predictive power of activations decreasing with layer depth.
This trend suggests that neural networks normalize perturbations introduced by data augmentations.

\paragraph{Tuning Pipelines for Deep Learning Kernels}
Automatic machine learning program optimization is becoming a fruitful area of research for both machine learning algorithms and downstream optimization tasks such as NAS and graph optimization.
However, one major impediment to researchers currently working in the field is the lack of a reusable system stack spanning predefined search spaces to device-specific code generation and transparent runtimes for benchmarking implementations and prototyping optimization pipelines.
We presented SeaNet, which aims to fill this gap by enabling machine learning researchers to plug in their custom optimization algorithms into challenging optimization tasks.
We presented two typical problem settings: the optimization of a collection of kernels corresponding to deep learning workloads, and peak-performance prediction for previously unseen kernels.
Additionally, we give a detailed description of a device-portable RPC system that enables users to quickly move between different hardware devices without affecting their algorithm implementation.
Finally, we presented evaluations of modular implementations for the tasks on various hardware devices to highlight the flexibility of the SeaNet environment.

\paragraph{Quality and Bandwidth for Multi-Resolution Image Storage}
Faced with growing demand for storage, image hosting services are increasingly turning to dynamic image resizing to improve the efficiency of image storage.
We showed that progressive encodings can dramatically reduce the amount of data that needs to be read for resizing images---potentially saving over 80\% of read bandwidth when tuned encode-time parameters used.
Finally, we give an estimate of progressive JPEG decode overheads which suggests that while serving custom progressive images directly to energy-constrained devices is difficult to justify, transcoding custom progressive JPEG on the server side incurs acceptable overheads.

\paragraph{End-to-end Optimization for Resolution}
Image resolution is a fundamental hyperparameter in computer vision with ties to compute complexity, operator optimizations, and storage bandwidth.
The best choice of resolution also depends on other choices such as crop sizes and their effect on the distribution of object scales.
We systematically characterized the dependencies and relationships between these tradeoffs, and describe methods for maximizing efficiency with respect to compute and storage cost, encompassing both cases where resolution is a static choice and dynamic at inference time.

