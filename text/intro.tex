\paragraph{Introduction}
Neural networks for computer vision present a tremendous challenge for systems practictioners and researchers.
In spite of a decade of constant progress in model architectures, perhaps the most frequent descriptor paired with neural networks is their data-hungry nature for training.
Yet, for popular models that achieve success in a given computer vision domain, the quantity of training data is eclipsed by the amount of data seen at inference time.

The sheer amount of data and computation required to train and execute neural networks has spawned ever-increasing efforts to tame the storage and compute appetite of neural networks.
Neural network models tax storage systems, as data for training and inference often takes the form of many relatively small files (e.g., images).
As models now typically contain hundreds of millions of parameters and billions of floating point operations, they also tax computer architectures and their designers and users, requiring careful orchestration of memory hierachies to maximize the utilization of hardware due to the bandwidth requirements of training and inference.

At a glance, neural networks depend on a tall stack of supporting technolgies to enable performance and developer productivity.
Fundamentally, achieving performant model implementations depends on highly specialized (often vendor-specific) linear algebra libraries (e.g., BLAS, MKLDNN, and cuDNN), as naive implementations of operators can trail libraries by several orders of magnitude.
Similarly, the de facto standard general purpose image training set ImageNet~\cite{russakovsky2015imagenet} comprises 1.2 million images.
To handle enable developer productivity on large datasets while maintaining hardware utilization, deep learning frameworks such as PyTorch~\cite{paszke2019pytorch} handle tasks such as automatic differentiation, parallel data augmentation preprocessing, and model checkpointing, to name a few example of the breadth of software support required.
Additionally, the model architectures of computer vision models are themselves highly complex arrangements of deep learning operators---the lastest architectures are often the result of human insight paired with brute-force machine optimization~\cite{tan2019efficientnet, real2019regularized, liu2018darts}.
From just this superficial glance at the technology stack behind neural networks, we can infer that neural network pipelines for computer vision are staggeringly complex.
However, from this complexity, a rich set of opportunities for optimization emerges.
Peforming neural network inference touches nearly every aspect of system design and implementation, from storage (images and image formats), to computer architecture (for kernel or operator-level optimizations), with the neural network archiecture and model execution pipeline at the highest level.

Starting from the singular hyperparameter of image resolution, we show methods and opportunities for enabling efficient neural network inference at multiple levels of the system stack.
While image resolution itself is a basic concept, the choice of image resolution has a strong impact on system requirements throughout the neural network stack.
At the most basic level, the amount of data that must be stored and read scales with image resolution.
Model accuracy (generally) increases with image resolution.
Due to the typical design of current convolutional model architectures, computational complexity scales roughly quadratically with image resolution.

However, as we will see, each of these relationships hold nuances that either can enable favorable tradeoffs or prevent expected efficiency gains from materializing.
For example, while computational complexity may scale quadratically with image resolution, realizing computation savings in terms of wall-clock time may be difficult if reducing complexity also serves to reduce hardware utilization given a particular computer architecture and algorithm implementation.
On the opposite side, we may find that decreasing image resolution may increase model accuracy compared to existing approaches if we carefully compensate for the change in image scale.


\paragraph{The Issue of Scale Dependence}
From a neural network's perspective, image resolution, combined with the cropping or framing of object(s) in the image, determine the apparent scale of objects.
At training time, a typical approach is to apply data augmentation so that objects are presented at a wide range scales and contexts (with various occlusions of the object and background).
However, despite the effectiveness of data augmentation in improving a model's robustness to object scale, they remain sensitive to the distribution of object scales seen at training time.

This issue raises a fundamental question about neural network features.
If scale is an important attribute of objects, do models explicitly capture or encode scale in their intermediate representations?
If models indeed represent scale, then this property can potentially be leveraged to ``normalize'' the scale of objects ahead of time to improve inference performance.

While intermediate activations of neural networks are typically difficult to interpret directly, we can reveal the sensitivity of activations to object scales using a model pipeline that uses neural network activations (from a pretrained model) as input and an alternative training objective intended to rank the relative scales of input examples.

Using this pipeline, we can measure the predictive power of neural network activations on the ranking task to understand the extent to which activations encode scale, along with attributes corresponding to other data augmentations.
Additionally, we can weigh the relative contributions of activations from different layers to gauge which layers encode scale information the most.

As we will see later, if models capture scale information, this property enables them to discern the image resolution that maximizes inference accuracy.

\paragraph{Enabling Performance at Different Resolutions}
As alluded to previously, while the apparent scale of objects is modulated by the resolution and crop size of an input image, the true computational cost of inference given an image resolution depends on the implementation strategy of each operator in the model.
The relative efficiency of inference typically decreases with decreasing resolution, as hardware utilization falls and/or the share of computation that cannot be parallelized begins to dominate the total execution time.
While losses in efficiency can be partially recovered by crafting custom kernels for every operator and resolution combination, this process quickly becomes tedious and intractable given the large number of operators present in even a single model, resolution combination.
The difficulty in handcrafting implementations is amplified by the present diversity of hardware architectures and memory hierarchies: the optimal implementation strategy will vary widely between a CPU with main memory caches and specialized vector extensions and a GPU with a large register file and high-bandwidth scratchpad memories.
Recent work has proposed automatic compiler optimizations that use machine-learning guided search to rapidly comb through a wide range of candidate implementations, and we build on this effort to generate efficient implementations for each resolution, studying ways to reduce the time spent during the search process.

More concretely, the search process for generating efficient implementations can be broken into multiple phases that each have varying strategies.
We can separate the search process into a \emph{proposal} phase that selects potentially highly performat implementations based on prior knowledge, and a \emph{prediction} phase that estimates the performance of the proposed candidate implementations.


\paragraph{Enabling Storage Efficiency at Different Resolutions}
Exposing multiple resolutions as a hyperparameter can reduce computational complexity when low resolution inference is used, but wastes storage bandwidth if full images are loaded for every resolution.
A similar issue also occurs when storing and serving multiple resolution versions of images, especially for large datasets such as those found for social media services.
While the motivation for serving multiple resolution versions (images are shown in different contexts and devices vs. for different scales/model quality effects), the requirements are similar.
By leveraging properties of frequency domain representations that are common in popular image formats, we can specialize the data layout of images to match the desired resolutions.
Specializing the data layout requires only a single copy of each image, without redundant lossy copies for lower resoution or quality versions.
For neural network inference, this enables reading a different relative fraction of each image's total data for inference at a per-example granularity, without modification to the downstream neural network pipeline.


\paragraph{End-to-End Specialization for Resolution}
With an understanding of how the choice of image resolution can be used to tune model quality, and how to enable efficient inference (in terms of computation and storage), we study the impact of end-to-end specialization for image resolution at inference time.
As modern model architectures are agnostic to input resolution (when looking purely at input and operator shapes, rather than model quality), a natural framing of this specialization is the task of choosing the best resolution for each input image.
From a storage perspective, the objective is to balance resolution-adjacent choices such as the crop size and image quality of each image to minimize the amount of data that needs to be read from storage.
Additionally, we note the importance of specializing the implementation of each model for a given resolution.

We propose a two-model pipeline that first predicts the optimal resolution for inference using a lightweight model followed by a larger backbone model that performs inference at the selected resolution.
In tandem, the amount of data read from storage is calibrated to the model's preferences for image quality at each resolution, and the implementation of the model at each resolution is tuned to maximize utilization.

